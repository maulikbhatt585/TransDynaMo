{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "42efd74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "\n",
    "import transformers\n",
    "\n",
    "from model import TrajectoryModel\n",
    "from trajectory_gpt2 import GPT2Model\n",
    "\n",
    "import gymnasium as gym\n",
    "import argparse\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "9e4ee70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "b4772809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box([ -1.        -1.        -1.        -1.       -12.566371 -28.274334], [ 1.        1.        1.        1.       12.566371 28.274334], (6,), float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"Acrobot-v1\")\n",
    "print(env.observation_space)\n",
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "1abb7334",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Acrobot-v1\")\n",
    "\n",
    "n_traj = 100\n",
    "traj_states = torch.zeros((n_traj,traj_len,6))\n",
    "traj_actions = torch.zeros(n_traj,traj_len)\n",
    "traj_rewards = torch.zeros(n_traj,traj_len)\n",
    "\n",
    "state_dim = 6\n",
    "act_dim = 1\n",
    "traj_len = 500\n",
    "n = 0\n",
    "while n<n_traj:\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state)\n",
    "\n",
    "    for i in range(traj_len):\n",
    "        action = env.action_space.sample()\n",
    "        state_next, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        traj_actions[n,i] = action\n",
    "        traj_rewards[n,i] = reward\n",
    "        traj_states[n,i,:] = state\n",
    "\n",
    "        state = torch.tensor(state_next)\n",
    "\n",
    "        if (terminated or truncated) and (i<499):\n",
    "            n = n - 1\n",
    "            break\n",
    "            #state, info = env.reset()\n",
    "    env.close()\n",
    "    n = n + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "7f9ea911",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save( traj_states, \"traj_states.pt\")\n",
    "torch.save(traj_actions, \"traj_actions.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "f6a44c1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 500, 6])"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traj_states = torch.load(\"traj_states.pt\")\n",
    "traj_states.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "9361a266",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(batch_size, n_traj, traj_states, traj_actions):\n",
    "    idxs = random.choices(range(n_traj), k=batch_size)\n",
    "    batch_states = traj_states[idxs,:,:]\n",
    "    batch_actions = traj_actions[idxs,:]\n",
    "    \n",
    "    return batch_states, batch_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "ea3f1fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTransformer(TrajectoryModel):\n",
    "\n",
    "    \"\"\"\n",
    "    This model uses GPT to model (Return_1, state_1, action_1, Return_2, state_2, ...)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            state_dim,\n",
    "            act_dim,\n",
    "            hidden_size,\n",
    "            max_length=None,\n",
    "            max_ep_len=500,\n",
    "            action_tanh=True,\n",
    "            **kwargs\n",
    "    ):\n",
    "        super().__init__(state_dim, act_dim, max_length=max_length)\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        config = transformers.GPT2Config(\n",
    "            vocab_size=1,  # doesn't matter -- we don't use the vocab\n",
    "            n_embd=hidden_size,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        # note: the only difference between this GPT2Model and the default Huggingface version\n",
    "        # is that the positional embeddings are removed (since we'll add those ourselves)\n",
    "        self.transformer = GPT2Model(config)\n",
    "\n",
    "        self.embed_timestep = nn.Embedding(max_ep_len, hidden_size)\n",
    "        #self.embed_return = torch.nn.Linear(1, hidden_size)\n",
    "        self.embed_state = torch.nn.Linear(self.state_dim, hidden_size)\n",
    "        self.embed_action = torch.nn.Linear(self.act_dim, hidden_size)\n",
    "\n",
    "        self.embed_ln = nn.LayerNorm(hidden_size)\n",
    "\n",
    "        # note: we don't predict states or returns for the paper\n",
    "        self.predict_state = torch.nn.Linear(hidden_size, self.state_dim)\n",
    "        self.predict_action = nn.Sequential(\n",
    "            *([nn.Linear(hidden_size, self.act_dim)] + ([nn.Tanh()] if action_tanh else []))\n",
    "        )\n",
    "        self.predict_return = torch.nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, states, actions, timesteps, attention_mask=None):\n",
    "\n",
    "        batch_size, seq_length = states.shape[0], states.shape[1]\n",
    "\n",
    "        if attention_mask is None:\n",
    "            # attention mask for GPT: 1 if can be attended to, 0 if not\n",
    "            attention_mask = torch.ones((batch_size, seq_length), dtype=torch.long)\n",
    "\n",
    "        # embed each modality with a different head\n",
    "        state_embeddings = self.embed_state(states)\n",
    "        action_embeddings = self.embed_action(actions)\n",
    "        #returns_embeddings = self.embed_return(returns_to_go)\n",
    "        time_embeddings = self.embed_timestep(timesteps)\n",
    "\n",
    "        # time embeddings are treated similar to positional embeddings\n",
    "        state_embeddings = state_embeddings + time_embeddings\n",
    "        action_embeddings = action_embeddings + time_embeddings\n",
    "        #returns_embeddings = returns_embeddings + time_embeddings\n",
    "        \n",
    "        #print(state_embeddings.size())\n",
    "        #print(action_embeddings.size())\n",
    "        #print(returns_embeddings.size())\n",
    "        \n",
    "#         print(torch.stack(\n",
    "#             (returns_embeddings, state_embeddings, action_embeddings), dim=1\n",
    "#         ).size())\n",
    "\n",
    "        # this makes the sequence look like (R_1, s_1, a_1, R_2, s_2, a_2, ...)\n",
    "        # which works nice in an autoregressive sense since states predict actions\n",
    "        stacked_inputs = torch.stack(\n",
    "            (state_embeddings, action_embeddings), dim=1\n",
    "        ).permute(0, 2, 1, 3).reshape(batch_size, 2*seq_length, self.hidden_size)\n",
    "        stacked_inputs = self.embed_ln(stacked_inputs)\n",
    "\n",
    "        # to make the attention mask fit the stacked inputs, have to stack it as well\n",
    "        stacked_attention_mask = torch.stack(\n",
    "            (attention_mask, attention_mask), dim=1\n",
    "        ).permute(0, 2, 1).reshape(batch_size, 2*seq_length)\n",
    "\n",
    "        # we feed in the input embeddings (not word indices as in NLP) to the model\n",
    "        transformer_outputs = self.transformer(\n",
    "            inputs_embeds=stacked_inputs,\n",
    "            attention_mask=stacked_attention_mask,\n",
    "        )\n",
    "        x = transformer_outputs['last_hidden_state']\n",
    "\n",
    "        # reshape x so that the second dimension corresponds to the original\n",
    "        # returns (0), states (1), or actions (2); i.e. x[:,1,t] is the token for s_t\n",
    "        x = x.reshape(batch_size, seq_length, 2, self.hidden_size).permute(0, 2, 1, 3)\n",
    "\n",
    "        # get predictions\n",
    "        #return_preds = self.predict_return(x[:,2])  # predict next return given state and action\n",
    "        state_preds = self.predict_state(x[:,1])    # predict next state given state and action\n",
    "        action_preds = self.predict_action(x[:,0])  # predict next action given state\n",
    "\n",
    "        return state_preds, action_preds\n",
    "\n",
    "    def get_state(self, states, actions, timesteps, **kwargs):\n",
    "        # we don't care about the past rewards in this model\n",
    "\n",
    "        states = states.reshape(1, -1, self.state_dim)\n",
    "        actions = actions.reshape(1, -1, self.act_dim)\n",
    "        timesteps = timesteps.reshape(1, -1)\n",
    "\n",
    "        if self.max_length is not None:\n",
    "            states = states[:,-self.max_length:]\n",
    "            actions = actions[:,-self.max_length:]\n",
    "            timesteps = timesteps[:,-self.max_length:]\n",
    "\n",
    "            # pad all tokens to sequence length\n",
    "            attention_mask = torch.cat([torch.zeros(self.max_length-states.shape[1]), torch.ones(states.shape[1])])\n",
    "            attention_mask = attention_mask.to(dtype=torch.long, device=states.device).reshape(1, -1)\n",
    "            states = torch.cat(\n",
    "                [torch.zeros((states.shape[0], self.max_length-states.shape[1], self.state_dim), device=states.device), states],\n",
    "                dim=1).to(dtype=torch.float32)\n",
    "            actions = torch.cat(\n",
    "                [torch.zeros((actions.shape[0], self.max_length - actions.shape[1], self.act_dim),\n",
    "                             device=actions.device), actions],\n",
    "                dim=1).to(dtype=torch.float32)\n",
    "            timesteps = torch.cat(\n",
    "                [torch.zeros((timesteps.shape[0], self.max_length-timesteps.shape[1]), device=timesteps.device), timesteps],\n",
    "                dim=1\n",
    "            ).to(dtype=torch.long)\n",
    "        else:\n",
    "            attention_mask = None\n",
    "\n",
    "        state_preds, action_preds = self.forward(\n",
    "            states, actions, timesteps, attention_mask=attention_mask, **kwargs)\n",
    "        \n",
    "#         print(state_preds.size())\n",
    "#         print(state_preds[0,-1].size())\n",
    "\n",
    "        return state_preds[0,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "37d78346",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTransformer(\n",
    "            state_dim=state_dim,\n",
    "            act_dim=act_dim,\n",
    "            max_length=20,\n",
    "            max_ep_len=500,\n",
    "            hidden_size=128,\n",
    "            n_layer=3,\n",
    "            n_head=1,\n",
    "            n_inner=4*128,\n",
    "            activation_function='relu',\n",
    "            n_positions=1024,\n",
    "            resid_pdrop=0.1,\n",
    "            attn_pdrop=0.1,\n",
    "            n_ctx=1500\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c55b4a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "328470ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/dt/pgvlk7nn3sz86f4g6dg_mgrm0000gn/T/ipykernel_17396/3576605738.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mstate_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_traj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraj_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraj_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mstate_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maction_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/dt/pgvlk7nn3sz86f4g6dg_mgrm0000gn/T/ipykernel_17396/2515718830.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, states, actions, timesteps, attention_mask)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# we feed in the input embeddings (not word indices as in NLP) to the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         transformer_outputs = self.transformer(\n\u001b[0m\u001b[1;32m     85\u001b[0m             \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacked_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacked_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Sem-4/CS444 Deep Learning for Computer Vision/decision-transformer/gym/decision_transformer/models/trajectory_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    731\u001b[0m                 )\n\u001b[1;32m    732\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m                 outputs = block(\n\u001b[0m\u001b[1;32m    734\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m                     \u001b[0mlayer_past\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_past\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Sem-4/CS444 Deep Learning for Computer Vision/decision-transformer/gym/decision_transformer/models/trajectory_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m     ):\n\u001b[0;32m--> 303\u001b[0;31m         attn_outputs = self.attn(\n\u001b[0m\u001b[1;32m    304\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0mlayer_past\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_past\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Sem-4/CS444 Deep Learning for Computer Vision/decision-transformer/gym/decision_transformer/models/trajectory_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0mpresent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         \u001b[0mattn_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Sem-4/CS444 Deep Learning for Computer Vision/decision-transformer/gym/decision_transformer/models/trajectory_gpt2.py\u001b[0m in \u001b[0;36m_attn\u001b[0;34m(self, q, k, v, attention_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=0.1,\n",
    "    weight_decay=1e-4,\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer,\n",
    "    lambda steps: min((steps+1)/10000, 1)\n",
    ")\n",
    "\n",
    "n_epochs = 1000\n",
    "\n",
    "losses = torch.zeros(n_epochs)\n",
    "\n",
    "for n in range(n_epochs):\n",
    "    state_batch, action_batch = get_batch(batch_size, n_traj, traj_states, traj_actions)\n",
    "\n",
    "    state_pred, action_preds = model.forward(state_batch, (action_batch.unsqueeze(-1)), torch.arange(0,500,1).unsqueeze(0))\n",
    "\n",
    "    loss = nn.MSELoss()\n",
    "    l = loss(state_pred,state_batch)\n",
    "    losses[n] = l.detach().cpu().item()\n",
    "    optimizer.zero_grad()\n",
    "    l.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "0409436c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 500])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states_i, actions_i = get_batch(1, n_traj, traj_states, traj_actions)\n",
    "actions_i.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "b803c38d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.9999,  0.0117,  0.9994, -0.0350, -0.0586,  0.0875]])\n",
      "tensor([[1.]])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "model.to(device=device)\n",
    "\n",
    "state = states_i[0,0,:]\n",
    "action = actions_i[0,0]\n",
    "\n",
    "# we keep all the histories on the device\n",
    "# note that the latest action and reward will be \"padding\"\n",
    "states = state.reshape(1, state_dim).to(device=device, dtype=torch.float32)\n",
    "print(states)\n",
    "actions = action.reshape(1, act_dim).to(device=device, dtype=torch.float32)\n",
    "print(actions)\n",
    "#rewards = torch.zeros(0, device=device, dtype=torch.float32)\n",
    "#target_return = torch.tensor(target_return, device=device, dtype=torch.float32)\n",
    "sim_states = []\n",
    "\n",
    "episode_return, episode_length = 0, 0\n",
    "for t in range(499):\n",
    "\n",
    "    # add padding\n",
    "    actions = torch.cat([actions, torch.zeros((1, act_dim), device=device)], dim=0)\n",
    "    #rewards = torch.cat([rewards, torch.zeros(1, device=device)])\n",
    "\n",
    "    pred_state = model.get_state(\n",
    "        states.to(dtype=torch.float32) ,\n",
    "        actions.to(dtype=torch.float32),\n",
    "        torch.arange(0,t+1,1).unsqueeze(0)\n",
    "    )\n",
    "    \n",
    "    actions[-1] = actions_i[0,t+1].reshape(1, act_dim)\n",
    "\n",
    "    cur_state = (pred_state).to(device=device).reshape(1, state_dim)\n",
    "    states = torch.cat([states, cur_state], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "70928b4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([500, 6])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "10ec6e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.4659, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = nn.MSELoss()\n",
    "l = loss(states.unsqueeze(0),states_i)\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca362c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e695b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
